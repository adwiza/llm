{
  "nodes": [
    {
      "id": "chatDeepseek_0",
      "position": {
        "x": 98.04006410256414,
        "y": 358.75320512820514
      },
      "type": "customNode",
      "data": {
        "id": "chatDeepseek_0",
        "label": "ChatDeepseek",
        "version": 1,
        "name": "chatDeepseek",
        "type": "chatDeepseek",
        "baseClasses": [
          "chatDeepseek",
          "BaseChatOpenAI",
          "BaseChatModel",
          "BaseLanguageModel",
          "Runnable"
        ],
        "category": "Chat Models",
        "description": "Wrapper around Deepseek large language models that use the Chat endpoint",
        "inputParams": [
          {
            "label": "Connect Credential",
            "name": "credential",
            "type": "credential",
            "credentialNames": [
              "deepseekApi"
            ],
            "id": "chatDeepseek_0-input-credential-credential",
            "display": true
          },
          {
            "label": "Model Name",
            "name": "modelName",
            "type": "asyncOptions",
            "loadMethod": "listModels",
            "default": "deepseek-chat",
            "id": "chatDeepseek_0-input-modelName-asyncOptions",
            "display": true
          },
          {
            "label": "Temperature",
            "name": "temperature",
            "type": "number",
            "step": 0.1,
            "default": 0.7,
            "optional": true,
            "id": "chatDeepseek_0-input-temperature-number",
            "display": true
          },
          {
            "label": "Streaming",
            "name": "streaming",
            "type": "boolean",
            "default": true,
            "optional": true,
            "additionalParams": true,
            "id": "chatDeepseek_0-input-streaming-boolean",
            "display": true
          },
          {
            "label": "Max Tokens",
            "name": "maxTokens",
            "type": "number",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatDeepseek_0-input-maxTokens-number",
            "display": true
          },
          {
            "label": "Top Probability",
            "name": "topP",
            "type": "number",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatDeepseek_0-input-topP-number",
            "display": true
          },
          {
            "label": "Frequency Penalty",
            "name": "frequencyPenalty",
            "type": "number",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatDeepseek_0-input-frequencyPenalty-number",
            "display": true
          },
          {
            "label": "Presence Penalty",
            "name": "presencePenalty",
            "type": "number",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatDeepseek_0-input-presencePenalty-number",
            "display": true
          },
          {
            "label": "Timeout",
            "name": "timeout",
            "type": "number",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatDeepseek_0-input-timeout-number",
            "display": true
          },
          {
            "label": "Stop Sequence",
            "name": "stopSequence",
            "type": "string",
            "rows": 4,
            "optional": true,
            "description": "List of stop words to use when generating. Use comma to separate multiple stop words.",
            "additionalParams": true,
            "id": "chatDeepseek_0-input-stopSequence-string",
            "display": true
          },
          {
            "label": "Base Options",
            "name": "baseOptions",
            "type": "json",
            "optional": true,
            "additionalParams": true,
            "description": "Additional options to pass to the Deepseek client. This should be a JSON object.",
            "id": "chatDeepseek_0-input-baseOptions-json",
            "display": true
          }
        ],
        "inputAnchors": [
          {
            "label": "Cache",
            "name": "cache",
            "type": "BaseCache",
            "optional": true,
            "id": "chatDeepseek_0-input-cache-BaseCache",
            "display": true
          }
        ],
        "inputs": {
          "cache": "",
          "modelName": "deepseek-chat",
          "temperature": "0.9",
          "streaming": true,
          "maxTokens": "",
          "topP": "",
          "frequencyPenalty": "",
          "presencePenalty": "",
          "timeout": "",
          "stopSequence": "",
          "baseOptions": ""
        },
        "outputAnchors": [
          {
            "id": "chatDeepseek_0-output-chatDeepseek-chatDeepseek|BaseChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable",
            "name": "chatDeepseek",
            "label": "chatDeepseek",
            "description": "Wrapper around Deepseek large language models that use the Chat endpoint",
            "type": "chatDeepseek | BaseChatOpenAI | BaseChatModel | BaseLanguageModel | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 580,
      "selected": false,
      "dragging": false,
      "positionAbsolute": {
        "x": 98.04006410256414,
        "y": 358.75320512820514
      }
    },
    {
      "id": "promptTemplate_0",
      "position": {
        "x": 116.06694158952178,
        "y": 985.5542153757724
      },
      "type": "customNode",
      "data": {
        "id": "promptTemplate_0",
        "label": "Prompt Template",
        "version": 1,
        "name": "promptTemplate",
        "type": "PromptTemplate",
        "baseClasses": [
          "PromptTemplate",
          "BaseStringPromptTemplate",
          "BasePromptTemplate",
          "Runnable"
        ],
        "category": "Prompts",
        "description": "Schema to represent a basic prompt for an LLM",
        "inputParams": [
          {
            "label": "Template",
            "name": "template",
            "type": "string",
            "rows": 4,
            "placeholder": "What is a good name for a company that makes {product}?",
            "id": "promptTemplate_0-input-template-string",
            "display": true
          },
          {
            "label": "Format Prompt Values",
            "name": "promptValues",
            "type": "json",
            "optional": true,
            "acceptVariable": true,
            "list": true,
            "id": "promptTemplate_0-input-promptValues-json",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "template": "Your task is to answer questions factually about a food menu, provided below and delimited by +++++. The user request is provided here: {request}\n\nStep 1: The first step is to check if the user is asking a question related to any type of food (even if that food item is not on the menu). If the question is about any type of food, we move on to Step 2 and ignore the rest of Step 1. If the question is not about food, then we send a response: \"Sorry! I cannot help with that. Please let me know if you have a question about our food menu.\"\n\nStep 2: In this step, we check that the user question is relevant to any of the items on the food menu. You should check that the food item exists in our menu first. If it doesn't exist then send a kind response to the user that the item doesn't exist in our menu and then include a list of available but similar food items without any other details (e.g., price). The food items available are provided below and delimited by +++++:\n\n+++++\nMenu: Kids Menu\nFood Item: Mini Cheeseburger\nPrice: $6.99\nVegan: N\nPopularity: 4/5\nIncluded: Mini beef patty, cheese, lettuce, tomato, and fries.\n\nMenu: Appetizers\nFood Item: Loaded Potato Skins\nPrice: $8.99\nVegan: N\nPopularity: 3/5\nIncluded: Crispy potato skins filled with cheese, bacon bits, and served with sour cream.\n\nMenu: Appetizers\nFood Item: Bruschetta\nPrice: $7.99\nVegan: Y\nPopularity: 4/5\nIncluded: Toasted baguette slices topped with fresh tomatoes, basil, garlic, and balsamic glaze.\n\nMenu: Main Menu\nFood Item: Grilled Chicken Caesar Salad\nPrice: $12.99\nVegan: N\nPopularity: 4/5\nIncluded: Grilled chicken breast, romaine lettuce, Parmesan cheese, croutons, and Caesar dressing.\n\nMenu: Main Menu\nFood Item: Classic Cheese Pizza\nPrice: $10.99\nVegan: N\nPopularity: 5/5\nIncluded: Thin-crust pizza topped with tomato sauce, mozzarella cheese, and fresh basil.\n\nMenu: Main Menu\nFood Item: Spaghetti Bolognese\nPrice: $14.99\nVegan: N\nPopularity: 4/5\nIncluded: Pasta tossed in a savory meat sauce made with ground beef, tomatoes, onions, and herbs.\n\nMenu: Vegan Options\nFood Item: Veggie Wrap\nPrice: $9.99\nVegan: Y\nPopularity: 3/5\nIncluded: Grilled vegetables, hummus, mixed greens, and a wrap served with a side of sweet potato fries.\n\nMenu: Vegan Options\nFood Item: Vegan Beyond Burger\nPrice: $11.99\nVegan: Y\nPopularity: 4/5\nIncluded: Plant-based patty, vegan cheese, lettuce, tomato, onion, and a choice of regular or sweet potato fries.\n\nMenu: Desserts\nFood Item: Chocolate Lava Cake\nPrice: $6.99\nVegan: N\nPopularity: 5/5\nIncluded: Warm chocolate cake with a gooey molten center, served with vanilla ice cream.\n\nMenu: Desserts\nFood Item: Fresh Berry Parfait\nPrice: $5.99\nVegan: Y\nPopularity: 4/5\nIncluded: Layers of mixed berries, granola, and vegan coconut yogurt.\n+++++\n\nStep 3: If the item exists in our food menu and the user is requesting specific information, provide that relevant information to the user using the food menu. Make sure to use a friendly tone and keep the response concise.\n\nPerform the following reasoning steps to send a response to the user:\nStep 1: < Step 1 reasoning >\nStep 2: < Step 2 reasoning >\n\nResponse to the user (only output the final response): <response to user>\n\nExtract the final response from the text delimited by without reasoning.\n\nOnly output what comes after \"Response to the user:\".",
          "promptValues": "{\"request\":\"{{question}}\"}"
        },
        "outputAnchors": [
          {
            "id": "promptTemplate_0-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable",
            "name": "promptTemplate",
            "label": "PromptTemplate",
            "description": "Schema to represent a basic prompt for an LLM",
            "type": "PromptTemplate | BaseStringPromptTemplate | BasePromptTemplate | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 519,
      "selected": false,
      "positionAbsolute": {
        "x": 116.06694158952178,
        "y": 985.5542153757724
      },
      "dragging": false
    },
    {
      "id": "llmChain_0",
      "position": {
        "x": 693.7825455914627,
        "y": 944.0864223804319
      },
      "type": "customNode",
      "data": {
        "id": "llmChain_0",
        "label": "LLM Chain",
        "version": 3,
        "name": "llmChain",
        "type": "LLMChain",
        "baseClasses": [
          "LLMChain",
          "BaseChain",
          "Runnable"
        ],
        "category": "Chains",
        "description": "Chain to run queries against LLMs",
        "inputParams": [
          {
            "label": "Chain Name",
            "name": "chainName",
            "type": "string",
            "placeholder": "Name Your Chain",
            "optional": true,
            "id": "llmChain_0-input-chainName-string",
            "display": true
          }
        ],
        "inputAnchors": [
          {
            "label": "Language Model",
            "name": "model",
            "type": "BaseLanguageModel",
            "id": "llmChain_0-input-model-BaseLanguageModel",
            "display": true
          },
          {
            "label": "Prompt",
            "name": "prompt",
            "type": "BasePromptTemplate",
            "id": "llmChain_0-input-prompt-BasePromptTemplate",
            "display": true
          },
          {
            "label": "Output Parser",
            "name": "outputParser",
            "type": "BaseLLMOutputParser",
            "optional": true,
            "id": "llmChain_0-input-outputParser-BaseLLMOutputParser",
            "display": true
          },
          {
            "label": "Input Moderation",
            "description": "Detect text that could generate harmful output and prevent it from being sent to the language model",
            "name": "inputModeration",
            "type": "Moderation",
            "optional": true,
            "list": true,
            "id": "llmChain_0-input-inputModeration-Moderation",
            "display": true
          }
        ],
        "inputs": {
          "model": "{{chatDeepseek_0.data.instance}}",
          "prompt": "{{promptTemplate_0.data.instance}}",
          "outputParser": "",
          "inputModeration": "",
          "chainName": "extraction"
        },
        "outputAnchors": [
          {
            "name": "output",
            "label": "Output",
            "type": "options",
            "description": "",
            "options": [
              {
                "id": "llmChain_0-output-llmChain-LLMChain|BaseChain|Runnable",
                "name": "llmChain",
                "label": "LLM Chain",
                "description": "",
                "type": "LLMChain | BaseChain | Runnable"
              },
              {
                "id": "llmChain_0-output-outputPrediction-string|json",
                "name": "outputPrediction",
                "label": "Output Prediction",
                "description": "",
                "type": "string | json"
              }
            ],
            "default": "llmChain"
          }
        ],
        "outputs": {
          "output": "outputPrediction"
        },
        "selected": false
      },
      "width": 300,
      "height": 514,
      "selected": false,
      "positionAbsolute": {
        "x": 693.7825455914627,
        "y": 944.0864223804319
      },
      "dragging": false
    },
    {
      "id": "promptTemplate_1",
      "position": {
        "x": 1153.3525020727564,
        "y": 1160.167385838278
      },
      "type": "customNode",
      "data": {
        "id": "promptTemplate_1",
        "label": "Prompt Template (1)",
        "version": 1,
        "name": "promptTemplate",
        "type": "PromptTemplate",
        "baseClasses": [
          "PromptTemplate",
          "BaseStringPromptTemplate",
          "BasePromptTemplate",
          "Runnable"
        ],
        "category": "Prompts",
        "description": "Schema to represent a basic prompt for an LLM",
        "inputParams": [
          {
            "label": "Template",
            "name": "template",
            "type": "string",
            "rows": 4,
            "placeholder": "What is a good name for a company that makes {product}?",
            "id": "promptTemplate_1-input-template-string",
            "display": true
          },
          {
            "label": "Format Prompt Values",
            "name": "promptValues",
            "type": "json",
            "optional": true,
            "acceptVariable": true,
            "list": true,
            "id": "promptTemplate_1-input-promptValues-json",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "template": "Perform the following refinement steps on the final output delimited by ###.\n\n1). Shortern the text to one sentence\n2). User a friendly tone\n\n\n###\n{final_response}\n###",
          "promptValues": "{\"final_response\":\"{{llmChain_0.data.instance}}\"}"
        },
        "outputAnchors": [
          {
            "id": "promptTemplate_1-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable",
            "name": "promptTemplate",
            "label": "PromptTemplate",
            "description": "Schema to represent a basic prompt for an LLM",
            "type": "PromptTemplate | BaseStringPromptTemplate | BasePromptTemplate | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 519,
      "selected": false,
      "positionAbsolute": {
        "x": 1153.3525020727564,
        "y": 1160.167385838278
      },
      "dragging": false
    },
    {
      "id": "llmChain_1",
      "position": {
        "x": 1559.5005141667702,
        "y": 616.1791313374706
      },
      "type": "customNode",
      "data": {
        "id": "llmChain_1",
        "label": "LLM Chain (1)",
        "version": 3,
        "name": "llmChain",
        "type": "LLMChain",
        "baseClasses": [
          "LLMChain",
          "BaseChain",
          "Runnable"
        ],
        "category": "Chains",
        "description": "Chain to run queries against LLMs",
        "inputParams": [
          {
            "label": "Chain Name",
            "name": "chainName",
            "type": "string",
            "placeholder": "Name Your Chain",
            "optional": true,
            "id": "llmChain_1-input-chainName-string",
            "display": true
          }
        ],
        "inputAnchors": [
          {
            "label": "Language Model",
            "name": "model",
            "type": "BaseLanguageModel",
            "id": "llmChain_1-input-model-BaseLanguageModel",
            "display": true
          },
          {
            "label": "Prompt",
            "name": "prompt",
            "type": "BasePromptTemplate",
            "id": "llmChain_1-input-prompt-BasePromptTemplate",
            "display": true
          },
          {
            "label": "Output Parser",
            "name": "outputParser",
            "type": "BaseLLMOutputParser",
            "optional": true,
            "id": "llmChain_1-input-outputParser-BaseLLMOutputParser",
            "display": true
          },
          {
            "label": "Input Moderation",
            "description": "Detect text that could generate harmful output and prevent it from being sent to the language model",
            "name": "inputModeration",
            "type": "Moderation",
            "optional": true,
            "list": true,
            "id": "llmChain_1-input-inputModeration-Moderation",
            "display": true
          }
        ],
        "inputs": {
          "model": "{{chatDeepseek_0.data.instance}}",
          "prompt": "{{promptTemplate_1.data.instance}}",
          "outputParser": "",
          "inputModeration": "",
          "chainName": ""
        },
        "outputAnchors": [
          {
            "name": "output",
            "label": "Output",
            "type": "options",
            "description": "",
            "options": [
              {
                "id": "llmChain_1-output-llmChain-LLMChain|BaseChain|Runnable",
                "name": "llmChain",
                "label": "LLM Chain",
                "description": "",
                "type": "LLMChain | BaseChain | Runnable"
              },
              {
                "id": "llmChain_1-output-outputPrediction-string|json",
                "name": "outputPrediction",
                "label": "Output Prediction",
                "description": "",
                "type": "string | json"
              }
            ],
            "default": "llmChain"
          }
        ],
        "outputs": {
          "output": "llmChain"
        },
        "selected": false
      },
      "width": 300,
      "height": 514,
      "selected": false,
      "positionAbsolute": {
        "x": 1559.5005141667702,
        "y": 616.1791313374706
      },
      "dragging": false
    }
  ],
  "edges": [
    {
      "source": "chatDeepseek_0",
      "sourceHandle": "chatDeepseek_0-output-chatDeepseek-chatDeepseek|BaseChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable",
      "target": "llmChain_0",
      "targetHandle": "llmChain_0-input-model-BaseLanguageModel",
      "type": "buttonedge",
      "id": "chatDeepseek_0-chatDeepseek_0-output-chatDeepseek-chatDeepseek|BaseChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable-llmChain_0-llmChain_0-input-model-BaseLanguageModel"
    },
    {
      "source": "promptTemplate_0",
      "sourceHandle": "promptTemplate_0-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable",
      "target": "llmChain_0",
      "targetHandle": "llmChain_0-input-prompt-BasePromptTemplate",
      "type": "buttonedge",
      "id": "promptTemplate_0-promptTemplate_0-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable-llmChain_0-llmChain_0-input-prompt-BasePromptTemplate"
    },
    {
      "source": "llmChain_0",
      "sourceHandle": "llmChain_0-output-outputPrediction-string|json",
      "target": "promptTemplate_1",
      "targetHandle": "promptTemplate_1-input-promptValues-json",
      "type": "buttonedge",
      "id": "llmChain_0-llmChain_0-output-outputPrediction-string|json-promptTemplate_1-promptTemplate_1-input-promptValues-json"
    },
    {
      "source": "chatDeepseek_0",
      "sourceHandle": "chatDeepseek_0-output-chatDeepseek-chatDeepseek|BaseChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable",
      "target": "llmChain_1",
      "targetHandle": "llmChain_1-input-model-BaseLanguageModel",
      "type": "buttonedge",
      "id": "chatDeepseek_0-chatDeepseek_0-output-chatDeepseek-chatDeepseek|BaseChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable-llmChain_1-llmChain_1-input-model-BaseLanguageModel"
    },
    {
      "source": "promptTemplate_1",
      "sourceHandle": "promptTemplate_1-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable",
      "target": "llmChain_1",
      "targetHandle": "llmChain_1-input-prompt-BasePromptTemplate",
      "type": "buttonedge",
      "id": "promptTemplate_1-promptTemplate_1-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable-llmChain_1-llmChain_1-input-prompt-BasePromptTemplate"
    }
  ]
}